---
title: "Home Credit Modeling"
author: "Pablo Zarate"
date: "3/26/2023"
output:   
  html_document:
    toc: true
    theme: united
---
# Introduction

People with bad or no credit have a difficult time getting a personal, home, or business loan. For folks who do want a loan, they are often preyed upon by lenders who push exorbitant fees and annual percentage rates. 

Home Credit hopes to solve this problem by targeting their business model on customers who have bad or no credit history without using predatory lending practices. To do this, Home Credit needs to find a way to predict how capable each applicant is with loan payments. 

In this modeling assignment, we will explore the 'application_train' dataset for Home Credit and use Random Forest to predict which customers are likely to have an issue in paying their loan.

Using classification, we'll attempt to create a model with the reflective accuracy and ROC-AUC to help us predict which customers are likely to have issues in paying their loan.

From this submission, I will apply any feedback given to improve this model and collaborate on a final analysis with fellow students in this course for a presentation  on April 18, 2023. 

# Data Preparation 

Our data preparation uses what was completed in the EDA assignment - converting some variables to factor variables and removing columns with a significant proportion of variables being NA.

I originally wanted to use installment payments as a dataset as well, but faced issues in joining the dataset. Once the dataset was joined, I had more than 1,000,000 observations in the dataset - causing models to take more than an hour to process. Ulitimately, I decided against using the installments payments data. The code chunk below has traces of importing and cleaning that file.
```{r Data Preparation}
# Loading packages
library(janitor)
library(tictoc)
tic()
library(psych)
library(tidyverse)
library(dplyr)
library(tidyr)
library(AppliedPredictiveModeling)
library(caret)
library(rpart)
library(rpart.plot)
library(readxl)
library(tidymodels)
library(ranger)
library(themis)
library(yardstick)

# Import files, set stringAsFactors = FALSE
#print(getwd())
application_train <- read.csv(file = "application_train.csv", stringsAsFactors = FALSE)
#installments_payments <- read.csv(file = "installments_payments.csv", stringsAsFactors = FALSE)

# We will convert this data to factor variables
application_train$NAME_CONTRACT_TYPE <- factor(application_train$NAME_CONTRACT_TYPE)
application_train$CODE_GENDER <- factor(application_train$CODE_GENDER)
application_train$FLAG_OWN_CAR <- factor(application_train$FLAG_OWN_CAR)
application_train$FLAG_OWN_REALTY <- factor(application_train$FLAG_OWN_REALTY)
application_train$NAME_TYPE_SUITE <- factor(application_train$NAME_TYPE_SUITE)
application_train$NAME_INCOME_TYPE <- factor(application_train$NAME_INCOME_TYPE)
application_train$NAME_EDUCATION_TYPE <- factor(application_train$NAME_EDUCATION_TYPE)
application_train$NAME_FAMILY_STATUS <- factor(application_train$NAME_FAMILY_STATUS)
application_train$NAME_HOUSING_TYPE <- factor(application_train$NAME_HOUSING_TYPE)
application_train$OCCUPATION_TYPE <- factor(application_train$OCCUPATION_TYPE)
application_train$WEEKDAY_APPR_PROCESS_START <- factor(application_train$WEEKDAY_APPR_PROCESS_START)
application_train$ORGANIZATION_TYPE <- factor(application_train$ORGANIZATION_TYPE)
application_train$FONDKAPREMONT_MODE <- factor(application_train$FONDKAPREMONT_MODE)
application_train$HOUSETYPE_MODE <- factor(application_train$HOUSETYPE_MODE)
application_train$WALLSMATERIAL_MODE <- factor(application_train$WALLSMATERIAL_MODE)
application_train$EMERGENCYSTATE_MODE <- factor(application_train$EMERGENCYSTATE_MODE)
application_train$FLAG_MOBIL <- as.factor(application_train$FLAG_MOBIL)
application_train$FLAG_EMP_PHONE <- as.factor(application_train$FLAG_EMP_PHONE)
application_train$FLAG_WORK_PHONE <- as.factor(application_train$FLAG_WORK_PHONE)
application_train$FLAG_CONT_MOBILE <- as.factor(application_train$FLAG_CONT_MOBILE)
application_train$FLAG_PHONE <- as.factor(application_train$FLAG_PHONE)
application_train$FLAG_EMAIL <- as.factor(application_train$FLAG_EMAIL)
application_train$TARGET <- as.factor(application_train$TARGET)

#Cleaning names
at <- clean_names(application_train)
#ip <- clean_names(installments_payments)

# Removing columns from the dataframe due to substantial number of NAs. These are the normalized housing information columns.
at = subset(at, select =-c(ext_source_1, ext_source_3, apartments_avg, basementarea_avg, years_beginexpluatation_avg, years_build_avg, commonarea_avg, elevators_avg, entrances_avg, floorsmax_avg, floorsmin_avg, landarea_avg, livingapartments_avg, livingarea_avg, nonlivingapartments_avg, nonlivingarea_avg,apartments_mode, basementarea_mode,years_beginexpluatation_mode, years_build_mode, commonarea_mode, elevators_mode, entrances_mode, floorsmax_mode, floorsmin_mode, landarea_mode, livingapartments_mode, livingarea_mode, nonlivingapartments_mode, nonlivingarea_mode, apartments_medi, basementarea_medi, years_beginexpluatation_medi, years_build_medi, commonarea_medi, elevators_medi, entrances_medi, floorsmax_medi, floorsmin_medi, landarea_medi, livingapartments_medi, livingarea_medi, nonlivingapartments_medi, nonlivingarea_medi, totalarea_mode))

at1 <- at
at2 <- at1 

# We are splitting the data in two - the first dataset will remove all rows that have an NA field, while the second dataset will change null values to '0'. From there, we will run the Random Forest on both datasets and compare results. 

# First dataset (at1)
at1 <- na.omit(at1)

# First dataset with installments_payments (apps_pmts1)
#apps_pmts1 <- merge(x = at1, y = ip, by = "sk_id_curr")
#apps_pmts1 <- na.omit(apps_pmts1)

# # Second dataset (at2)
# at2$days_birth[is.na(at2$days_birth)] <-0
# at2$amt_credit[is.na(at2$amt_credit)] <-0      
# at2$amt_annuity[is.na(at2$amt_annuity)] <- 0  
# at2$days_employed[is.na(at2$days_employed)] <- 0       
# at2$amt_goods_price[is.na(at2$amt_goods_price)] <- 0 
# at2$days_id_publish[is.na(at2$days_id_publish)] <- 0   
# at2$own_car_age[is.na(at2$own_car_age)] <- 0 
# at2$amt_req_credit_bureau_day[is.na(at2$amt_req_credit_bureau_day)] <- 0     
# at2$amt_req_credit_bureau_hour[is.na(at2$amt_req_credit_bureau_hour)] <- 0       
# at2$amt_req_credit_bureau_week[is.na(at2$amt_req_credit_bureau_week)] <- 0        
# at2$amt_req_credit_bureau_mon[is.na(at2$amt_req_credit_bureau_mon)] <-0 
# at2$amt_req_credit_bureau_qrt[is.na(at2$amt_req_credit_bureau_qrt)] <- 0    
# at2$amt_req_credit_bureau_year[is.na(at2$amt_req_credit_bureau_year)] <- 0       
# at2$days_last_phone_change[is.na(at2$days_last_phone_change)] <- 0       
# at2$obs_30_cnt_social_circle[is.na(at2$obs_30_cnt_social_circle)] <- 0       
# at2$obs_60_cnt_social_circle[is.na(at2$obs_60_cnt_social_circle)] <- 0     
# at2$ext_source_2[is.na(at2$ext_source_2)] <- 0      
# at2$cnt_fam_members[is.na(at2$cnt_fam_members)] <- 0 
# at2$def_30_cnt_social_circle[is.na(at2$def_30_cnt_social_circle)] <- 0  
# at2$def_60_cnt_social_circle[is.na(at2$def_60_cnt_social_circle)] <- 0

# First dataset with installments_payments (apps_pmts2)
#apps_pmts2 <- merge(x = at2, y = ip, by = "sk_id_curr")
#apps_pmts2$amt_payment[is.na(apps_pmts2$amt_payment)] <- 0

#str(at1) #at1 - 92087 obs. of  77 variables
#str(at2) #at2 - 307511 obs. of  77 variables
#str(installments_payments) #13605401 obs. of  8 variables
#str(apps_pmts1) #3471679 obs. of  84 variables:
#str(apps_pmts2) #11591592 obs. of  84 variables

```

# Modeling Process: Candidate Models & Model Selection

I went into this project considering logistic regression and classification models to build a predictive model to find the target value '1' (customers with issues paying back their loan).

When running a logistic regression model, I was using only categorical variables in the model and seeing if adding or removing any variables any would cause a significant change in model accuracy. I also faced issues with NA data, as the logistic regression model wouldn't run if there were any NA's in the dataset.

From there, I pivoted to running a Random Forest model. I originally used a down sample function to try and compensate for the class imbalance in the target variables. However, this model produced a lower accuracy.

In the end, the Random Forest model was processed using the application_train dataset that removed all rows that had an 'NA'. Classification accuracy and the Area under the ROC Curve scores will be used to measure model & probability estimate performance. I am only using the name_education_type, amt_credit, flag_own_car and amt_income_total variables for this modeling assignment.

# Random Forest
```{r Random Forest}
#Splitting into training and testing dataset
split <- initial_split(at1, strata = target,
                                 prop = 0.7)
app_train <- training(split)
app_test <- testing(split)

# Reviewing porportions for train and test sets
app_train %>%
     count(target) %>%
     mutate(perc = n/sum(n)) # 0.93 vs. 0.069	

app_test %>%
     count(target) %>%
     mutate(perc = n/sum(n))# 0.93 vs. 0.069	

# Random Forest model
rf_mod <- 
  rand_forest(trees = 1000) %>% 
  set_engine("ranger") %>% 
  set_mode("classification")

set.seed(12345)
rf_fit <- 
  rf_mod %>% 
  fit(target ~ name_education_type +
                    amt_credit +
                      flag_own_car +
                      amt_income_total, data = app_train)

rf_training_pred <- 
  predict(rf_fit, app_train) %>% 
  bind_cols(predict(rf_fit, app_train, type = "prob")) %>% 
  bind_cols(app_train %>% 
              select(target))

# Training set predictions are optimistic - indicating a higher ROC_AUC than the testing model
rf_training_pred %>%                
  roc_auc(truth = target, .pred_0) #ROC: .86
rf_training_pred %>%                
  roc_auc(truth = target, .pred_1) #ROC: .13
rf_training_pred %>%             
  accuracy(truth = target, .pred_class) #Accuracy: 0.9302358

rf_testing_pred <- 
  predict(rf_fit, app_test) %>% 
  bind_cols(predict(rf_fit, app_test, type = "prob")) %>% 
  bind_cols(app_test %>% select(target))

# Testing set ROC_AUC and Accuracy
rf_testing_pred %>%                  
  roc_auc(truth = target, .pred_1) #ROC: 0.422209
rf_testing_pred %>%                 
  roc_auc(truth = target, .pred_0) #ROC: 0.577791
rf_testing_pred %>%                  
  accuracy(truth = target, .pred_class) #Accuracy: 0.9300684
```

# Modeling Process: Cross Validation

The training set statistics in this Random Forest model are optimistic. It memorizes the training set and re-predicts items from this same dataset, coming into nearly perfect results. 

I will use 10-fold cross-validation to re-sample the dataset and see if we can get a model with a ROC-AUC closer to the testing model.

```{r Random Forest: Cross Validation}
# Creating 10-fold cross-validation
set.seed(345)
folds <- vfold_cv(app_train, v = 10)
folds

rf_wf <- 
  workflow() %>%
  add_model(rf_mod) %>%
  add_formula(target ~ name_education_type +
                    amt_credit +
                      flag_own_car +
                      amt_income_total)

set.seed(456)
rf_fit_rs <- 
  rf_wf %>% 
  fit_resamples(folds)
rf_fit_rs

# The new value for ROC-AUC (0.5860582) is more realistic compared to the first output of the training model
collect_metrics(rf_fit_rs)
```
# Modeling Process: Model Tuning

Using tune() and cross-validation, we can specify which hyperparameters will be assessed through running a grid search.

At this point, I am running a new random forest model that takes hyperparameter tuning into account. However, I will only be tuning number of trees as assessing other hyperparameters (like minimum number of datapoints and mtry) is causing this model to take a long time to run (+30 minutes).
```{r Random Forest: Model Tuning}
# Initiating a new random forest model
app_recipe <- recipe(target ~ name_education_type +
                    amt_credit +
                      flag_own_car +
                      amt_income_total, data = app_train)

#' Using `rand_forest()` we specify the random forest algorithm and also the hyperparameters such as `trees` (the number of trees contained in a random forest). `tune()` specifies that each hyperparameter needs to be tuned though grid search.

model_app <- rand_forest(
  trees = tune()) %>%
  set_mode("classification") %>%
  set_engine("ranger")

#'Next, we aggregate the recipe and model using `workflow()`.`add_model` adds the model specified in the last step with the recipe that contains information about predictor and outcome variables and dummy coding.

app_wf <- workflow() %>%
  add_model(model_app) %>%
  add_recipe(app_recipe)

#' We next specify the cross validation process using `vfold_cv()`. `v=5` specifies that there will be 5 partitions of the training data set and each hyperparameter combinations performance will be assessed using this cross validation procedure. 

app_folds <- vfold_cv(app_train, v=5)

app_tune <- 
  app_wf %>% 
  tune_grid(
    resamples = app_folds,
    grid = 5
  ) 
```
# Modeling Process: Model Performance

Our final model has 1,957 trees, and an accuracy of .93 and ROC-AUC of .58, similar to the testing dataset. This model took 30 minutes to process.
```{r Modeling Process: Model Performance 7}
#We use `select_best("roc_auc")` to pick the best hyperparameter combination, which is 1957 trees.
best_model <- app_tune %>%
  select_best("roc_auc")

best_model 

final_workflow <- 
  app_wf %>% 
  finalize_workflow(best_model)

#' We fit the model using the best hyperparameter combination on the entire training set and then validate its performance on the test data. `last_fit` helps us accomplish this. `split = split` specifies that `split` contains information about test and train split performed earlier. Using `collect_metrics` we report the accuracy and roc_auc values. 

final_fit <- 
  final_workflow %>%
  last_fit(split = split) 

# We have an ROC_AUC of .5806702 which is similar to the testing dataset's estimate.
final_fit %>%
  collect_metrics()

final_tree<- extract_workflow(final_fit)
final_tree

```
# Results

I was able to create a predictive model using Random Forest that was close to Accuracy and ROC-AUC of the testing dataset. This model will be able to predict characteristics of a customer who may have issues paying their loan based off the information in this dataset. 

I believe it would have been more useful to use other datasets like installment payments or credit bureau information when running the final model. However, for the sake of time, we only used the application_train dataset. 

In conclusion, going forward I feel better equipped to identify which model should be ran for predictive purposes. I also feel like my coding skills have signficantly improved, as well as my interpretation of the metrics.

